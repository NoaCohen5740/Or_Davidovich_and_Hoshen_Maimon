# -*- coding: utf-8 -*-
"""Anomalies2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dyci7HgqlePBDR1A62430mC1pxTbxuOc

# Anomalies of the data:
"""

pip install lifelines

!pip install ucimlrepo

pip install prince

pip install scikit-fuzzy

# Imports
import pandas as pd
import numpy as np
from ucimlrepo import fetch_ucirepo
from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA, TruncatedSVD, FastICA
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score
from scipy.stats import kruskal, chi2_contingency, f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from lifelines import KaplanMeierFitter, statistics
import prince
import umap
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.mosaicplot import mosaic
from sklearn.metrics import pairwise_distances
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, kruskal
import math
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from matplotlib.colors import LinearSegmentedColormap

import warnings

"""The color palette:"""

# Blaugrana colors (hex values)
# deep burgundy and navy shades
blaugrana_palette = ["#A50044", "#004D98", "#8E0038", "#00295B"]

# Apply the palette globally
sns.set_style("whitegrid")
sns.set_palette(blaugrana_palette)

# Optional: preview the palette
sns.palplot(sns.color_palette(blaugrana_palette))
plt.title("Blaugrana Palette")
plt.show()

# Load the data
data = fetch_ucirepo(id=890)
X = data.data.features.copy()
y = data.data.targets.copy()

cid_series = y['cid']
time_series = X['time']
X = X.drop(columns=['time'])

categorical_cols = [col for col in X.columns if X[col].nunique() <= 10]
numeric_cols = [col for col in X.columns if col not in categorical_cols]
print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)

X.columns = X.columns.astype(str)

# Preprocessing pipeline
numeric_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols)
])

# Apply transformation
X_processed = preprocessor.fit_transform(X)
encoded_cat_columns = preprocessor.named_transformers_["cat"]["encoder"].get_feature_names_out(categorical_cols)
all_columns = numeric_cols + list(encoded_cat_columns)
X_std = pd.DataFrame(X_processed, columns=all_columns, index=X.index)

# Anomaly detection
iso_pred = IsolationForest(contamination=0.1, random_state=0).fit_predict(X_std)
lof_pred = LocalOutlierFactor(n_neighbors=20, contamination=0.1).fit_predict(X_std)
svm_pred = OneClassSVM(nu=0.1, gamma=0.1).fit(X_std).predict(X_std)
dbscan_pred = DBSCAN(eps=2.5, min_samples=5).fit_predict(X_std)
elliptic = EllipticEnvelope(contamination=0.05, random_state=42).fit_predict(X[numeric_cols])

# Combine votes
votes = np.array([
    (iso_pred == -1).astype(int),
    (lof_pred == -1).astype(int),
    (svm_pred == -1).astype(int),
    (dbscan_pred == -1).astype(int),
    (elliptic == -1).astype(int)
])

X['anomaly_votes'] = votes.sum(axis=0)
X['is_anomaly'] = (X['anomaly_votes'] >= 3).astype(int)

# Dimensionality reducers
reducers = {
    "PCA": PCA(n_components=2),
    "t-SNE": TSNE(n_components=2, random_state=42),
    "UMAP": umap.UMAP(n_components=2, random_state=42),
    "MDS": MDS(n_components=2, random_state=42),
    "Isomap": Isomap(n_components=2),
    "LLE": LocallyLinearEmbedding(n_components=2),
    "Spectral": SpectralEmbedding(n_components=2, random_state=42)
}

# Blaugrana palette for anomalies
blaugrana_palette = {0: "#004D98", 1: "#A50044"}  # blue for normal, burgundy for anomaly

# Plot all projections
fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(16, 24))
axes = axes.flatten()

for i, (name, reducer) in enumerate(reducers.items()):
    try:
        embedding = reducer.fit_transform(X_std)
        sns.scatterplot(
            x=embedding[:, 0],
            y=embedding[:, 1],
            hue=X['is_anomaly'],
            palette=blaugrana_palette,
            ax=axes[i],
            s=20,
            edgecolor=None
        )
        axes[i].set_title(f"{name} Projection")
        axes[i].set_xlabel("Component 1")
        axes[i].set_ylabel("Component 2")
        axes[i].legend(title="Anomaly", loc='best')
    except Exception as e:
        axes[i].text(0.5, 0.5, f"{name} Failed:\n{e}", ha='center', va='center')
        axes[i].set_title(f"{name} Projection")
        axes[i].set_axis_off()

plt.tight_layout()
plt.show()

"""We see that the best reduce of the anomalies is UMAP.

We will detect anomalies, visualize them and assessed survival
"""

# Preprocessing
X_processed = preprocessor.fit_transform(X)
encoded_cat_columns = preprocessor.named_transformers_["cat"]["encoder"].get_feature_names_out(categorical_cols)
all_columns = numeric_cols + list(encoded_cat_columns)
X_std = pd.DataFrame(X_processed, columns=all_columns, index=X.index)

# Anomaly Detection BEFORE reduction
iso_pred = IsolationForest(contamination=0.1, random_state=0).fit_predict(X_std)
lof_pred = LocalOutlierFactor(n_neighbors=20, contamination=0.1).fit_predict(X_std)
elliptic = EllipticEnvelope(contamination=0.05, random_state=42).fit_predict(X[numeric_cols])

# Dimensionality Reduction for DBSCAN and SVM
reduced_umap = umap.UMAP(n_components=2, random_state=42).fit_transform(X_std)


# Use reduced data for these:
svm_pred = OneClassSVM(nu=0.1, gamma=0.1).fit(reduced_umap).predict(reduced_umap)
dbscan_pred = DBSCAN(eps=2.5, min_samples=5).fit_predict(reduced_umap)

# Combine anomaly decisions (vote ≥ 3)
votes = np.array([
    (iso_pred == -1).astype(int),
    (lof_pred == -1).astype(int),
    (elliptic == -1).astype(int),
    (svm_pred == -1).astype(int),
    (dbscan_pred == -1).astype(int)
])
X['anomaly_votes'] = votes.sum(axis=0)
X['is_anomaly'] = (X['anomaly_votes'] >= 3).astype(int)

# UMAP Visualization
embedding_umap = umap.UMAP(n_components=2, random_state=42).fit_transform(X_std)
plt.figure(figsize=(8, 6))

# Define specific mapping for anomalies: 0 = normal, 1 = anomaly
blaugrana_anomaly_palette = {0: "#004D98", 1: "#A50044"}  # blue and burgundy
sns.scatterplot(
    x=embedding_umap[:, 0], y=embedding_umap[:, 1],
    hue=X['is_anomaly'], palette=blaugrana_anomaly_palette, s=20)

plt.title("UMAP Projection of Anomalies")
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.legend(title='Anomaly')
plt.tight_layout()
plt.show()

X_std['time'] = time_series
X_std['cid'] = cid_series
X['time'] = time_series
X['cid'] = cid_series



# Feature Importance for Anomaly Classification
X_model = X_std.copy()
y_model = X['is_anomaly']

# Import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier # Importing the necessary class

clf = RandomForestClassifier(random_state=0)
clf.fit(X_model, y_model)

importances = pd.Series(clf.feature_importances_, index=X_model.columns)
top_features = importances.sort_values(ascending=False).head(20)

plt.figure(figsize=(8, 6))
sns.barplot(x=top_features.values, y=top_features.index)
plt.title("Top Feature Importances for Anomaly Detection")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# Survival Analysis (if applicable)
if {'time'}.issubset(X.columns):
    kmf = KaplanMeierFitter()
    plt.figure(figsize=(8, 6))

    for label, group in X.groupby("is_anomaly"):
        kmf.fit(group['time'], label=f"Anomaly={label}")
        kmf.plot_survival_function()

    plt.title("Survival Curves: Anomaly vs Normal")
    plt.xlabel("Time")
    plt.ylabel("Survival Probability")
    plt.tight_layout()
    plt.show()

    # Log-rank test
    normal_group = X[X['is_anomaly'] == 0]
    anomaly_group = X[X['is_anomaly'] == 1]
    result = logrank_test(
        normal_group['time'], anomaly_group['time'],
    )
    print("Log-rank test p-value:", result.p_value)

# Define groups
normal_group = X[X['is_anomaly'] == 0]
anomaly_group = X[X['is_anomaly'] == 1]

# Initialize plot
plt.figure(figsize=(8, 6))
kmf = KaplanMeierFitter()

# Fit and plot normal group
kmf.fit(durations=normal_group['time'], event_observed=normal_group['cid'], label="Normal")
kmf.plot_survival_function()

# Fit and plot anomaly group
kmf.fit(durations=anomaly_group['time'], event_observed=anomaly_group['cid'], label="Anomalous")
kmf.plot_survival_function()

# Finalize plot
plt.title("Kaplan–Meier Survival Curves (with Censoring)")
plt.xlabel("Time (days)")
plt.ylabel("Survival Probability")
plt.grid(True)
plt.tight_layout()
plt.show()

# Log-rank test
logrank_result = logrank_test(
    normal_group['time'], anomaly_group['time'],
    event_observed_A=normal_group['cid'],
    event_observed_B=anomaly_group['cid']
)
print("Log-rank test p-value (with censoring):", logrank_result.p_value)

# Assuming you have a column 'is_anomaly' in your DataFrame X
# where 1 = anomaly, 0 = normal

num_anomalies = X['is_anomaly'].sum()
total_samples = len(X)
anomaly_percentage = (num_anomalies / total_samples) * 100

# Print results
print(f"Number of anomalies: {num_anomalies}")
print(f"Total number of samples: {total_samples}")
print(f"Anomaly percentage: {anomaly_percentage:.1f}%")

# Generate text for report
print("\n--- For report inclusion (English) ---")
print(f"Anomalous samples represent approximately {anomaly_percentage:.1f}% of the dataset "
      f"(N = {num_anomalies} out of {total_samples} total patients).")

"""To test for differences between abnormal and non-abnormal patients, we will use several statistical tests:"""

# Define feature groups
cont_features = X.select_dtypes(include=np.number).columns.drop(['anomaly_votes', 'is_anomaly'], errors='ignore')
cat_features = categorical_cols

# Containers for results
t_test_results = []
mannwhitney_results = []
chi2_results = []

# Continuous Variables: t-test and Mann–Whitney
print("\n=== t-test and Mann–Whitney U Test Results (Continuous Features) ===")
for col in cont_features:
    try:
        normal = X[X['is_anomaly'] == 0][col].dropna()
        anomaly = X[X['is_anomaly'] == 1][col].dropna()

        if len(normal) >= 2 and len(anomaly) >= 2 and normal.nunique() > 1 and anomaly.nunique() > 1:
            t_stat, t_p = ttest_ind(normal, anomaly, equal_var=False)
            u_stat, u_p = mannwhitneyu(normal, anomaly, alternative='two-sided')

            t_test_results.append({'Feature': col, 'p-value': t_p})
            mannwhitney_results.append({'Feature': col, 'p-value': u_p})
        else:
            t_test_results.append({'Feature': col, 'p-value': np.nan})
            mannwhitney_results.append({'Feature': col, 'p-value': np.nan})
    except Exception as e:
        t_test_results.append({'Feature': col, 'p-value': np.nan, 'Note': str(e)})
        mannwhitney_results.append({'Feature': col, 'p-value': np.nan, 'Note': str(e)})

t_df = pd.DataFrame(t_test_results).sort_values("p-value")
u_df = pd.DataFrame(mannwhitney_results).sort_values("p-value")

print("\n--- t-test ---")
print(t_df)

print("\n--- Mann–Whitney U ---")
print(u_df)

# Categorical Variables: Chi-squared Test
print("\n=== Chi-squared Test Results (Categorical Features) ===")
for col in cat_features:
    try:
        contingency = pd.crosstab(X[col], X['is_anomaly'])
        chi2, p, _, _ = chi2_contingency(contingency)
        chi2_results.append({'Feature': col, 'p-value': p})
    except Exception as e:
        chi2_results.append({'Feature': col, 'p-value': np.nan, 'Note': str(e)})

chi2_df = pd.DataFrame(chi2_results).sort_values("p-value")

print("\n--- Chi-squared ---")
print(chi2_df)

"""We want to see for each variable the differences when there is anomaly and when there is no anomaly:"""

# Visualize Top Feature Differences via Boxplots
top_vars = ['wtkg', 'cd40', 'cd820', 'cd80', 'cd420']  # based on earlier feature importance

# Define number of plots per row
cols = 3
rows = math.ceil(len(top_vars) / cols)

fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))
# Flatten in case of single row/column
axes = axes.flatten()

for i, col in enumerate(top_vars):
    if col in X.columns:
        sns.boxplot(x='is_anomaly', y=col, data=X, ax=axes[i])
        axes[i].set_title(f"{col} Distribution by Anomaly Status")
        axes[i].set_xlabel("Anomaly")
        axes[i].set_ylabel(col)
    else:
        # Hide unused subplot if col is not in X
        axes[i].axis('off')

# Hide any remaining empty subplots
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

"""Show the distribution of time:"""

# Assuming 'time_series' is your time data
plt.figure(figsize=(10, 6))
sns.histplot(time_series, kde=True)  # Use a histogram with kernel density estimation
plt.title('Distribution of Time')
plt.xlabel('Time')
plt.ylabel('Frequency')
plt.show()

# Anomaly detection based only on 'time'
iso = IsolationForest(contamination=0.05, random_state=42)
X['time_anomaly'] = iso.fit_predict(X[['time']])
X['time_anomaly'] = (X['time_anomaly'] == -1).astype(int)

# Plot distribution of 'time' with anomalies highlighted
plt.figure(figsize=(10, 6))
sns.histplot(data=X[X['time_anomaly'] == 1], x='time', hue='time_anomaly', bins=50)
plt.title('Distribution of Time with Anomalies Highlighted')
plt.xlabel('Time')
plt.ylabel('Frequency')
plt.legend(title='Anomaly', labels=['Normal', 'Anomaly'])
plt.tight_layout()
plt.show()

# Statistical tests to compare anomaly vs non-anomaly groups:

# Define relevant numeric and categorical columns
numeric_cols = ['age', 'wtkg', 'preanti', 'cd40', 'cd420', 'cd80', 'cd820']
categorical_cols = ['trt', 'hemo', 'homo', 'drugs', 'karnof', 'oprior', 'z30', 'race',
                    'gender', 'str2', 'strat', 'symptom', 'treat', 'offtrt', 'cid']

print("\n--- Kruskal-Wallis Test for Numeric Variables ---")
for col in numeric_cols:
    group1 = X[X['time_anomaly'] == 0][col]
    group2 = X[X['time_anomaly'] == 1][col]
    stat, p = kruskal(group1, group2)
    print(f"{col:<10} → H = {stat:.2f}, p = {p:.4f}")

print("\n--- Chi-Squared Test for Categorical Variables ---")
for col in categorical_cols:
    table = pd.crosstab(X['time_anomaly'], X[col])
    if table.shape[0] == 2 and table.shape[1] > 1:
        chi2, p, _, _ = chi2_contingency(table)
        print(f"{col:<10} → Chi² = {chi2:.2f}, p = {p:.4f}")

# Detect time anomalies using Isolation Forest
iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
anomaly_labels = iso.fit_predict(time_series.values.reshape(-1, 1))  # Changed line
X['anomaly_time'] = (anomaly_labels == -1).astype(int)

# Identify categorical and numeric features
categorical_cols = [col for col in X.columns if X[col].nunique() <= 10 and X[col].dtype in [np.int64, np.int32]]
numeric_cols = [col for col in X.columns if col not in categorical_cols + ['anomaly_time', 'cid']]

# Perform statistical tests
results = []

for col in numeric_cols:
    group0 = X[X['anomaly_time'] == 0][col]
    group1 = X[X['anomaly_time'] == 1][col]
    H_stat, p_kruskal = kruskal(group0, group1)
    F_stat, p_anova = f_oneway(group0, group1)
    u_stat, p_mw = mannwhitneyu(group0, group1, alternative='two-sided')
    results.extend([
        {'Feature': col, 'Test': 'Kruskal-Wallis', 'Statistic': H_stat, 'p-value': p_kruskal},
        {'Feature': col, 'Test': 'ANOVA', 'Statistic': F_stat, 'p-value': p_anova},
        {'Feature': col, 'Test': 'Mann-Whitney U', 'Statistic': u_stat, 'p-value': p_mw},
    ])

for col in categorical_cols:
    contingency = pd.crosstab(X['anomaly_time'], X[col])
    if contingency.shape[0] > 1 and contingency.shape[1] > 1:
        chi2, p, _, _ = chi2_contingency(contingency)
        results.append({
            'Feature': col,
            'Test': 'Chi-squared',
            'Statistic': chi2,
            'p-value': p
        })

# Convert results to DataFrame
results_df = pd.DataFrame(results)
print(results_df.sort_values(by=['Feature', 'Test']))

"""Statistical test on cid:"""

# Assume that the variables X['time'] and X['cid'] already exist
# Descriptive statistics by groups
summary_stats = X.groupby('cid')['time'].agg(['count', 'mean', 'median', 'std', 'min', 'max'])
print("Summary statistics for 'time' by 'cid':\n", summary_stats)

# Statistical tests between groups
group_0 = X[X['cid'] == 0]['time']
group_1 = X[X['cid'] == 1]['time']

# T-test
t_pval = ttest_ind(group_0, group_1, equal_var=False).pvalue
# Mann–Whitney
u_pval = mannwhitneyu(group_0, group_1, alternative='two-sided').pvalue

print(f"\nT-test p-value: {t_pval:.4f}")
print(f"Mann–Whitney U test p-value: {u_pval:.4f}")

# Kaplan–Meier curve
kmf = KaplanMeierFitter()
plt.figure(figsize=(8, 6))

for label, group in X.groupby('cid'):
    kmf.fit(durations=group['time'], event_observed=(group['cid']==1), label=f"cid={label}")
    kmf.plot_survival_function()

plt.title("Kaplan–Meier Curves by cid (Event: Treatment Failure)")
plt.xlabel("Time (days)")
plt.ylabel("Treatment Retention Probability")
plt.grid(True)
plt.tight_layout()
plt.show()

# Log-rank test
cid0 = X[X['cid'] == 0]
cid1 = X[X['cid'] == 1]
logrank = logrank_test(cid0['time'], cid1['time'], event_observed_A=cid0['cid'], event_observed_B=cid1['cid'])
print(f"Log-rank test p-value: {logrank.p_value:.4f}")

# Boxplot + ECDF
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.boxplot(x='cid', y='time', data=X)
plt.title("Boxplot of Time by cid")
plt.xlabel("cid (0 = Censored, 1 = Failure)")
plt.ylabel("Time")

plt.subplot(1, 2, 2)
sns.ecdfplot(data=X, x='time', hue='cid')
plt.title("ECDF of Time by cid")
plt.xlabel("Time")
plt.ylabel("Cumulative Probability")

plt.tight_layout()
plt.show()

# Optional: Cox Proportional Hazards (if features exist)
if 'cd4' in X.columns:
    df_cox = X[['time', 'cid', 'cd4']].dropna()
    cph = CoxPHFitter()
    cph.fit(df_cox, duration_col='time', event_col='cid')
    cph.print_summary()
    cph.plot()
    plt.show()

"""Time by offtrt:"""

# Frequency table between cid and offtrt
contingency = pd.crosstab(X['offtrt'], X['cid'])
chi2, p_chi2, _, _ = chi2_contingency(contingency)
print("Chi-squared p-value (offtrt vs cid):", p_chi2)

# Effect of offtrt on time
off0 = X[X['offtrt'] == 0]['time']
off1 = X[X['offtrt'] == 1]['time']

t_p = ttest_ind(off0, off1, equal_var=False).pvalue
u_p = mannwhitneyu(off0, off1).pvalue

print(f"T-test p (time ~ offtrt): {t_p:.4f}")
print(f"Mann–Whitney U p (time ~ offtrt): {u_p:.4f}")

# Boxplot + ECDF
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.boxplot(x='offtrt', y='time', data=X)
plt.title("Boxplot of Time by offtrt")

plt.subplot(1, 2, 2)
sns.ecdfplot(data=X, x='time', hue='offtrt')
plt.title("ECDF of Time by offtrt")
plt.tight_layout()
plt.show()

# Kaplan–Meier by offtrt
kmf = KaplanMeierFitter()
plt.figure(figsize=(8, 6))
for val, group in X.groupby('offtrt'):
    kmf.fit(group['time'], group['cid'], label=f"offtrt={val}")
    kmf.plot_survival_function()
plt.title("KM Curves by offtrt")
plt.xlabel("Time (days)")
plt.ylabel("Treatment Retention Probability")
plt.tight_layout()
plt.show()

# Frequency table of treatment failures offtrt
cross = pd.crosstab(X['offtrt'], X['cid'], normalize='index') * 100
print(cross)

# Apply multiple anomaly detection methods
iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42).fit_predict(X[numeric_cols])
svm = OneClassSVM(nu=0.05, kernel='rbf').fit_predict(X[numeric_cols])
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05).fit_predict(X[numeric_cols])
elliptic = EllipticEnvelope(contamination=0.05, random_state=42).fit_predict(X[numeric_cols])

# Convert anomaly labels to binary (1 = anomaly, 0 = normal)
methods = {
    'IsolationForest': (iso == -1).astype(int),
    'OneClassSVM': (svm == -1).astype(int),
    'LocalOutlierFactor': (lof == -1).astype(int),
    'EllipticEnvelope': (elliptic == -1).astype(int)
}

# Combine results and label as anomaly only if majority vote
anomaly_votes = pd.DataFrame(methods)
X['anomaly'] = (anomaly_votes.sum(axis=1) >= 3).astype(int)
print(X['anomaly'].value_counts()/len(X)*100)

# Ensure 'anomaly' and 'time' exist in the data
if 'anomaly' in X.columns and 'time' in X.columns:
    # Create a list of parameters to visualize
    parameters_to_plot = [col for col in X.columns if col not in ['anomaly', 'cid']]

    # Determine layout
    n_cols = 3
    n_rows = (len(parameters_to_plot) + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))
    axes = axes.flatten()

    for i, col in enumerate(parameters_to_plot):
        if X[col].nunique() > 10:
            sns.kdeplot(data=X, x=col, hue='anomaly', ax=axes[i], fill=True)
        else:
            sns.countplot(data=X, x=col, hue='anomaly', ax=axes[i])
        axes[i].set_title(f'{col} Distribution by Anomaly Status')

    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

"""Reducing by MCA+PCA"""

# MCA on categorical
mca = prince.MCA(n_components=10, random_state=42).fit(X[categorical_cols])
mca_coords = mca.row_coordinates(X[categorical_cols])
mca_coords.columns = mca_coords.columns.astype(str)

# Combine MCA with numeric
combined = pd.concat([
    mca_coords.reset_index(drop=True),
    X[numeric_cols].reset_index(drop=True)
], axis=1)
X_scaled = StandardScaler().fit_transform(combined)

# PCA on the combined MCA+numeric
X_pca = PCA(n_components=2, random_state=42).fit_transform(X_scaled)

# Define custom Blaugrana color map
blaugrana_palette_map = {False: "#004D98", True: "#A50044"}

# Plot with custom palette
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=X_pca[:, 0],
    y=X_pca[:, 1],
    hue=X['anomaly'].astype(bool),
    palette=blaugrana_palette_map
)
plt.title('MCA+PCA Reduced Data with Anomaly Detection')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend(title='Anomaly')
plt.tight_layout()
plt.show()

# Kruskal-Wallis for numeric
X['time'] = time_series
X['cid'] = cid_series
print("\n Kruskal-Wallis Test (Numeric Features) Anomaly vs Non-Anomaly:\n")
for col in numeric_cols + ['time']:
    stat, p = kruskal(X[X['anomaly'] == 0][col], X[X['anomaly'] == 1][col])
    print(f"{col:<20} → H = {stat:.2f}, p = {p:.4f}")

# Chi-Squared for categorical
print("\n Chi-Squared Test (Categorical Features) Anomaly vs Non-Anomaly:\n")
for col in categorical_cols + ['cid']:
    table = pd.crosstab(X['anomaly'], X[col])
    if table.shape[0] > 1 and table.shape[1] > 1:
        chi2, p, _, _ = chi2_contingency(table)
        print(f"{col:<20} → Chi² = {chi2:.2f}, p = {p:.4f}")

# Tukey post-hoc test for time
print("Tukey HSD post-hoc for 'time':")
tukey_df = X[['time', 'anomaly']].copy()
tukey_df['anomaly'] = tukey_df['anomaly'].astype(str)
tukey = pairwise_tukeyhsd(endog=tukey_df['time'], groups=tukey_df['anomaly'], alpha=0.05)
print(tukey.summary())

# Kaplan-Meier Plot
kmf = KaplanMeierFitter()
plt.figure(figsize=(10, 6))

for group in [0, 1]:
    ix = X['anomaly'] == group
    kmf.fit(X.loc[ix, 'time'], X.loc[ix, 'cid'], label=f"Anomaly: {bool(group)}")
    kmf.plot_survival_function(ci_show=True)

plt.title("Kaplan-Meier Curve: Anomaly vs Non-Anomaly")
plt.xlabel("Time to Event")
plt.ylabel("Survival Probability")
plt.grid(True)
plt.tight_layout()
plt.show()

# Heatmap of Means
means_df = X.groupby('anomaly')[numeric_cols + ['time']].mean()

# Define a custom colormap from white to Blaugrana burgundy red
blaugrana_red_cmap = LinearSegmentedColormap.from_list("blaugrana_red", ["#ffffff", "#A50044"])

# Plot the heatmap using the red-themed Blaugrana colormap
plt.figure(figsize=(10, 6))
sns.heatmap(means_df.T, annot=True, fmt=".1f", cmap=blaugrana_red_cmap, cbar_kws={'label': 'Mean'})
plt.title("Mean Feature Values by Anomaly Status")
plt.xlabel("Anomaly")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""Numeric variables by anomalies"""

# Custom palette for anomaly: 0 = blue, 1 = red
blaugrana_anomaly_palette = {'0': "#004D98", '1': "#A50044"}

# Boxplots for Numeric Variables by Anomaly
numeric_to_plot = numeric_cols + ['time']
n_cols = 3
n_rows = -(-len(numeric_to_plot) // n_cols)  # Ceiling division

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5 * n_rows))
axes = axes.flatten()

for i, col in enumerate(numeric_to_plot):
    sns.boxplot(
        data=X,
        x='anomaly',
        y=col,
        ax=axes[i],
        palette=blaugrana_anomaly_palette
    )
    axes[i].set_title(f'{col} by Anomaly')
    axes[i].set_xlabel('Anomaly')
    axes[i].set_ylabel(col)

# Turn off unused axes
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.suptitle("Boxplots of Numeric Features by Anomaly", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()

"""Categorial variables by anomalies:"""

# Custom palette for anomaly: 0 = blue, 1 = red
blaugrana_anomaly_palette = {0: "#004D98", 1: "#A50044"}

# Countplots for Categorical Features
cat_to_plot = categorical_cols + ['cid']
n_cols = 3
n_rows = -(-len(cat_to_plot) // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(cat_to_plot):
    sns.countplot(
        data=X,
        x=col,
        hue='anomaly',
        ax=axes[i],
        palette=blaugrana_anomaly_palette
    )
    axes[i].set_title(f'{col} by Anomaly')
    axes[i].legend(title='Anomaly')

# Turn off unused axes
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.suptitle("Categorical Distributions by Anomaly", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()

"""Calculate the total number and percentage of anomalies and normal samples in the dataset:


"""

# General anomaly count and percentage
total = len(X)
anomaly_count = X['anomaly'].sum()
normal_count = total - anomaly_count
anomaly_percentage = anomaly_count / total * 100

print(f"Total samples: {total}")
print(f"Anomalies: {anomaly_count} ({anomaly_percentage:.2f}%)")
print(f"Non-anomalies: {normal_count} ({100 - anomaly_percentage:.2f}%)\n")

# Anomaly count breakdown for each variable

print("Anomaly Distribution by each Variable:\n")

# Categorical variables
for col in categorical_cols + ['cid']:
    print(f"--- {col} ---")
    ct = pd.crosstab(X[col], X['anomaly'], margins=True)
    ct.columns = ['Normal', 'Anomaly', 'Total']
    print(ct)
    print()

# Numeric variables (mean/median comparison)
for col in numeric_cols + ['time']:
    anomaly_vals = X[X['anomaly'] == 1][col]
    normal_vals = X[X['anomaly'] == 0][col]
    print(f"--- {col} ---")
    print(f"Mean (Anomaly)     : {anomaly_vals.mean():.2f}")
    print(f"Mean (Non-Anomaly) : {normal_vals.mean():.2f}")
    print(f"Median (Anomaly)   : {anomaly_vals.median():.2f}")
    print(f"Median (Non-Anomaly): {normal_vals.median():.2f}")
    print(f"N (Anomaly)        : {anomaly_vals.count()}")
    print(f"N (Non-Anomaly)    : {normal_vals.count()}\n")

#ANOVA test:
print("ANOVA Test (Numeric Features) Anomaly vs. Non-Anomaly\n")
for col in numeric_cols + ['time']:
    group0 = X[X['anomaly'] == 0][col]
    group1 = X[X['anomaly'] == 1][col]
    stat, p = f_oneway(group0, group1)
    print(f"{col:<20} → F = {stat:.2f}, p = {p:.4f}")

warnings.filterwarnings("ignore")

# Apply MCA and combine with numeric
mca = prince.MCA(n_components=10, random_state=42).fit(X[categorical_cols])
mca_coords = mca.row_coordinates(X[categorical_cols])
combined = pd.concat([mca_coords.reset_index(drop=True), X[numeric_cols].reset_index(drop=True)], axis=1)
combined.columns = combined.columns.astype(str) # Convert all column names to strings
X_scaled = StandardScaler().fit_transform(combined)

# UMAP Reduction
numeric_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])


categorical_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols)
])

X_processed = preprocessor.fit_transform(X)

encoded_cat_columns = preprocessor.named_transformers_["cat"]["encoder"].get_feature_names_out(categorical_cols)
all_columns = numeric_cols + list(encoded_cat_columns)
X_processed_df = pd.DataFrame(X_processed, columns=all_columns, index=X.index)
X_umap = umap.UMAP(n_components=6, n_neighbors=30, min_dist=0, random_state=42).fit_transform(X_processed)

# Anomaly Detection Methods
methods = {
    "Isolation Forest": IsolationForest(n_estimators=100, contamination=0.05, random_state=42),
    "One-Class SVM": OneClassSVM(nu=0.05, gamma='scale'),
    "Local Outlier Factor": LocalOutlierFactor(n_neighbors=20, contamination=0.05, novelty=True),
    "Elliptic Envelope": EllipticEnvelope(contamination=0.05, random_state=42)
}

results = {}
for name, model in methods.items():
    model.fit(X_scaled)
    labels = model.predict(X_scaled)
    anomalies = (labels == -1).astype(int)
    results[name] = anomalies

# Sub-clustering anomalies using GMM on UMAP
iso_anomalies = results["Isolation Forest"]
anomaly_points = X_umap[iso_anomalies == 1]
kmeans_sub = KMeans(n_clusters=2, random_state=42).fit(anomaly_points)
sub_cluster_labels = kmeans_sub.labels_

# Count anomalies (5% validation)
total_samples = len(X)
anomaly_counts = {name: int(np.sum(label_array)) for name, label_array in results.items()}
anomaly_percentages = {name: round(100 * count / total_samples, 2) for name, count in anomaly_counts.items()}

# Save output
output_summary = {
    "Anomaly Counts": anomaly_counts,
    "Anomaly Percentages": anomaly_percentages,
    "Subcluster Counts (Isolation Forest)": dict(pd.Series(sub_cluster_labels).value_counts().sort_index())
}

output_summary

"""The stability of different clusteing algorithms:"""

X_scaled = X_umap

# Clustering algorithms
clustering_algos = {
    'KMeans': KMeans(n_clusters=2, random_state=42),
    'GMM': GaussianMixture(n_components=2, random_state=42),
    'Agglomerative': AgglomerativeClustering(n_clusters=2),
    'Spectral': SpectralClustering(n_clusters=2, random_state=42)
}

# Clustering Stability
def clustering_stability(X, algo, n=5):
    preds = []
    for i in range(n):
        model = algo
        labels = model.fit_predict(X) if not hasattr(model, 'predict') else model.fit(X).predict(X)
        preds.append(labels)
    aris = [adjusted_rand_score(preds[i], preds[i+1]) for i in range(n-1)]
    nmis = [normalized_mutual_info_score(preds[i], preds[i+1]) for i in range(n-1)]
    return np.mean(aris), np.mean(nmis)

results = {}
for name, algo in clustering_algos.items():
    ari, nmi = clustering_stability(X_scaled, algo)
    results[name] = {'Adjusted Rand Index': ari, 'Normalized Mutual Info': nmi}
print("\n=== Clustering Stability ===")
print(pd.DataFrame(results).T.round(3))

# Risk Profiles of Anomalies
profile_summary = X.groupby('anomaly')[numeric_cols].agg(['mean', 'std']).round(2)
print("\n=== Risk Profiles by Anomaly Status ===")
print(profile_summary)