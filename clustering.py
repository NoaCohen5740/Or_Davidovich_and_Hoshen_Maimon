# -*- coding: utf-8 -*-
"""clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14hTubkLkfiFeSKMY7LJxuC43lsCHP4VO

# Clustering of the data:
"""

pip install ucimlrepo

pip install prince

pip install scikit-fuzzy

pip install umap-learn

# imports
from ucimlrepo import fetch_ucirepo
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats.mstats import winsorize

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, TruncatedSVD, FastICA
from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from itertools import product
import umap
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from scipy.stats import kruskal, chi2_contingency
from itertools import combinations
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.stats.contingency_tables import Table2x2
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from joblib import Parallel, delayed
from sklearn.decomposition import IncrementalPCA
from mpl_toolkits.mplot3d import Axes3D


import warnings

# Blaugrana colors (hex values)
# deep burgundy and navy shades
blaugrana_palette = ["#A50044", "#004D98", "#8E0038", "#00295B"]

# Apply the palette globally
sns.set_style("whitegrid")
sns.set_palette(blaugrana_palette)

# Optional: preview the palette
sns.palplot(sns.color_palette(blaugrana_palette))
plt.title("Blaugrana Palette")
plt.show()

# Load dataset
aids_data = fetch_ucirepo(id=890)
X = aids_data.data.features.copy()
y = aids_data.data.targets.copy()
X.columns = X.columns.astype(str)

# Identify column types
categorical_cols = [col for col in X.columns if X[col].nunique() <= 10]
numeric_cols = [col for col in X.columns if X[col].nunique() > 10]

# Remove outliers from numeric features using IQR
def replace_outliers_with_bound(data):
    for col in numeric_cols:  # Iterate over numeric columns
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Replace outliers with nearest bound for the current column
        data.loc[data[col] < lower_bound, col] = lower_bound
        data.loc[data[col] > upper_bound, col] = upper_bound

    return data

X = replace_outliers_with_bound(X)
cid_series = y['cid']
time_series = X['time']
X = X.drop(columns=['time'])
numeric_cols.remove('time')
# Define preprocessing pipeline
numeric_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols)
])

# Apply transformation
X_processed = preprocessor.fit_transform(X)

# Optional: return as DataFrame
encoded_cat_columns = preprocessor.named_transformers_["cat"]["encoder"].get_feature_names_out(categorical_cols)
all_columns = numeric_cols + list(encoded_cat_columns)
X_processed_df = pd.DataFrame(X_processed, columns=all_columns, index=X.index)

# Dimensionality reduction
def apply_dr(X, method, n_components,x_cat):
    if method == 'PCA':
        return PCA(n_components=n_components).fit_transform(X)
    elif method == 'UMAP':
        return umap.UMAP(n_components=n_components, random_state=42).fit_transform(X)
    elif method == 'FAMD':
        return prince.FAMD(n_components=n_components, random_state=42).fit(x_cat).row_coordinates(x_cat).values
    elif method == 'MCA+PCA':
        mca = prince.MCA(n_components=10, random_state=42).fit(x_cat)
        mca_coords = mca.row_coordinates(x_cat)
        return PCA(n_components=n_components).fit_transform(mca_coords)

    else:
        raise ValueError(f"Unknown or unsupported DR method: {method}")
def get_cluster_sizes(labels):
  if isinstance(labels, np.ndarray):
    labels = pd.Series(labels)
  cluster_counts = labels.value_counts().reset_index()
  cluster_counts.columns = ['Cluster', 'Size']
  return cluster_counts

# Clustering
def cluster(X, method, k):
    if method == 'KMeans':
        return KMeans(n_clusters=k, random_state=42).fit_predict(X)
    elif method == 'GMM':
        return GaussianMixture(n_components=k, random_state=42).fit(X).predict(X)
    elif method == 'Agglomerative':
        return AgglomerativeClustering(n_clusters=k).fit_predict(X)
    elif method == 'Spectral':
        return SpectralClustering(n_clusters=k, affinity='nearest_neighbors', random_state=42).fit_predict(X)
    elif method == 'DBSCAN':
        return DBSCAN(eps=0.5, min_samples=5).fit_predict(X)
    else:
        raise ValueError("Unsupported clustering method")

"""Comparing between the clustering algorithms:"""

# Methods
dr_methods = ['PCA', 'UMAP', 'FAMD','MCA+PCA']

cluster_methods = ['KMeans', 'GMM', 'Spectral', 'DBSCAN', 'Agglomerative']
k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]
n_values = [2,3]
# Run comparisons
results = []
for n in n_values:
  for k in k_values:
      for dr_method, clust_method in product(dr_methods, cluster_methods):
          try:
              X_reduced = apply_dr(X_processed_df, dr_method, n, X)
              x_scaled = StandardScaler().fit_transform(X_reduced)
              labels = cluster(X_reduced, clust_method, k)
              if get_cluster_sizes(labels).iloc[0, 1] < 300 or get_cluster_sizes(labels).iloc[0,1]>2000: continue  # Skip small clusters

              silhouette = silhouette_score(X_reduced, labels)
              ch_score = calinski_harabasz_score(X_reduced, labels)
              db_score = davies_bouldin_score(X_reduced, labels)

              results.append({
                  'k': k,
                  'DR_Method': dr_method,
                  'Clusterer': clust_method,
                  'Silhouette': silhouette,
                  'Calinski_Harabasz': ch_score,
                  'Davies_Bouldin': db_score,
                  'n': n
              })
          except Exception as e:
              print(f" {dr_method} + {clust_method} (k={k}) failed: {e}")

# Final results
df_results = pd.DataFrame(results)
print(df_results.sort_values(by="Silhouette", ascending=False).head(10))
print(df_results.sort_values(by="Calinski_Harabasz", ascending=False).head(10))
print(df_results.sort_values(by="Davies_Bouldin", ascending=True).head(10))

"""We saw there are a lot of similarity, we choose to use GMM for next codes and research.

Visualize with UMAP and GMM because we saw erliear they are the best.
"""

def visualize_clusters(X_reduced, labels, dr_method, cluster_method, k, n):
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')

    unique_labels = np.unique(labels)
    for label in unique_labels:
        ax.scatter(X_reduced[labels == label, 0],
                   X_reduced[labels == label, 1],
                   label=f'Cluster {label}')

    ax.set_title(f'{dr_method} + {cluster_method} (k={k}, n={n})')
    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_zlabel('Dimension 3')
    ax.legend()
    plt.show()

k = 2
n = 3
dr_method = 'UMAP'
cluster_method = 'GMM'

X_umap = umap.UMAP(n_components=6, n_neighbors=30, min_dist=0, random_state=42).fit_transform(X_processed)
X_reduced = X_umap
# Cluster the reduced data
labels = cluster(X_reduced, cluster_method, k)
visualize_clusters(X_reduced, labels, dr_method, cluster_method, k, n)

"""Evaluation of clustering quality:"""

# Evaluate metrics for different UMAP dimensions
dims = [2, 3, 4, 5,6,7,8,9,10]
silhouette_scores = []
calinski_scores = []
davies_scores = []

for n in dims:
    reducer = umap.UMAP(n_components=n, random_state=42)
    X_umap = reducer.fit_transform(X_processed_df)
    labels = GaussianMixture(n_components=2, random_state=42).fit_predict(X_umap)

    silhouette_scores.append(silhouette_score(X_umap, labels))
    calinski_scores.append(calinski_harabasz_score(X_umap, labels))
    davies_scores.append(davies_bouldin_score(X_umap, labels))

# Plot metrics
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.lineplot(x=dims, y=silhouette_scores, marker="o", ax=axes[0])
axes[0].set_title("Silhouette Score by UMAP Dimension")
axes[0].set_xlabel("UMAP Dimensions")
axes[0].set_ylabel("Silhouette Score")

sns.lineplot(x=dims, y=calinski_scores, marker="o", ax=axes[1])
axes[1].set_title("Calinski-Harabasz Index by UMAP Dimension")
axes[1].set_xlabel("UMAP Dimensions")
axes[1].set_ylabel("Calinski-Harabasz Index")

sns.lineplot(x=dims, y=davies_scores, marker="o", ax=axes[2])
axes[2].set_title("Davies-Bouldin Index by UMAP Dimension")
axes[2].set_xlabel("UMAP Dimensions")
axes[2].set_ylabel("Davies-Bouldin Index")

plt.tight_layout()
plt.show()

# Tests the different performance of the cluster in relation to other parameters in umap
# Evaluate metrics for different UMAP parameters (n_neighbors, min_dist)
n_neighbors_values = [5, 10, 15, 20, 30]
min_dist_values = [0.0, 0.1, 0.2, 0.3, 0.5]

results_umap_params = []

for n_neighbors, min_dist in product(n_neighbors_values, min_dist_values):
    try:
        reducer = umap.UMAP(n_components=6, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)
        X_umap = reducer.fit_transform(X_processed_df)
        labels = GaussianMixture(n_components=2, random_state=42).fit_predict(X_umap)

        silhouette = silhouette_score(X_umap, labels)
        ch_score = calinski_harabasz_score(X_umap, labels)
        db_score = davies_bouldin_score(X_umap, labels)

        results_umap_params.append({
            'n_neighbors': n_neighbors,
            'min_dist': min_dist,
            'Silhouette': silhouette,
            'Calinski_Harabasz': ch_score,
            'Davies_Bouldin': db_score
        })

    except Exception as e:
        print(f" UMAP (n_neighbors={n_neighbors}, min_dist={min_dist}) failed: {e}")

df_umap_params = pd.DataFrame(results_umap_params)
print(df_umap_params.sort_values(by="Silhouette", ascending=False).head(10))
print(df_umap_params.sort_values(by="Calinski_Harabasz", ascending=False).head(10))
print(df_umap_params.sort_values(by="Davies_Bouldin", ascending=True).head(10))

"""ARI and NMI for clustering algorithms:"""

# Dimensionality Reduction with UMAP
X_umap = umap.UMAP(n_components=6, n_neighbors=30, min_dist=0, random_state=42).fit_transform(X_processed)

# Apply Clustering Algorithms
kmeans_labels = KMeans(n_clusters=2, random_state=42).fit_predict(X_umap)
gmm_labels = GaussianMixture(n_components=2, random_state=42).fit(X_umap).predict(X_umap)
agg_labels = AgglomerativeClustering(n_clusters=2).fit_predict(X_umap)
spectral_labels = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=42).fit_predict(X_umap)

# Create Results DataFrame
results = pd.DataFrame({
    'KMeans': kmeans_labels,
    'GMM': gmm_labels,
    'Agglomerative': agg_labels,
    'Spectral': spectral_labels
})

# Calculate Agreement Metrics
methods = results.columns.tolist()
ari_matrix = pd.DataFrame(index=methods, columns=methods)
nmi_matrix = pd.DataFrame(index=methods, columns=methods)

for m1 in methods:
    for m2 in methods:
        ari_matrix.loc[m1, m2] = adjusted_rand_score(results[m1], results[m2])
        nmi_matrix.loc[m1, m2] = normalized_mutual_info_score(results[m1], results[m2])

print("Adjusted Rand Index Matrix:")
print(ari_matrix)

print("\nNormalized Mutual Information Matrix:")
print(nmi_matrix)

"""see the variables (numeric and categorial) vs the clusters
Calculate the categorical distributions by cluster
see the mean & std of numeric features by cluster:

"""

n=6
X_reduced = X_umap
labels = cluster(X_reduced, cluster_method, k) # Cluster the reduced data
X['cluster'] = labels
X['cid'] = cid_series
X['time'] = time_series


# Plot numeric distributions
numeric_to_plot = numeric_cols + ['time']
rows = int(np.ceil(len(numeric_to_plot) / 3))
fig, axes = plt.subplots(rows, 3, figsize=(16, 5 * rows))
axes = axes.flatten()

for i, col in enumerate(numeric_to_plot):
    sns.boxplot(data=X, x='cluster', y=col, ax=axes[i])
    axes[i].set_title(f'{col} by Cluster')

for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

# Plot categorical distributions
cat_to_plot = categorical_cols + ['cid']
rows = int(np.ceil(len(cat_to_plot) / 3))
fig, axes = plt.subplots(rows, 3, figsize=(16, 4 * rows))
axes = axes.flatten()

for i, col in enumerate(cat_to_plot):
    sns.countplot(data=X, x=col, hue='cluster', ax=axes[i])
    axes[i].set_title(f'{col} by Cluster')
    axes[i].legend(title='Cluster')

for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

# Summary Tables

# Numeric summary (mean/std)
print("\nMean & Std of Numeric Features by Cluster:\n")
print(X.groupby('cluster')[numeric_cols + ['time']].agg(['mean', 'std']))

# Categorical proportions
print("\nCategorical Distributions by Cluster (Proportions):\n")
for col in categorical_cols + ['cid']:
    print(f"\n== {col} ==")
    print(pd.crosstab(X['cluster'], X[col], normalize='index').round(2))

cluster_sizes_df = get_cluster_sizes(X['cluster'])
cluster_sizes_df

"""Compare clusters across numeric and categorical variables statistically:"""

# Identify columns
meta_cols = ['time', 'cid']

# Kruskal-Wallis tests for numeric features
print("Kruskal-Wallis Test for Numeric Features by Cluster:\n")
for col in numeric_cols + ['time']:
    groups = [X[X['cluster'] == c][col].dropna() for c in X['cluster'].unique()]
    stat, p = kruskal(*groups)
    print(f"{col:<20} → H = {stat:.2f}, p = {p:.4f}")

# Chi-squared tests for categorical features (including cid)
print("\nChi-Squared Test for Categorical Features by Cluster:\n")
for col in categorical_cols + ['cid']:
    table = pd.crosstab(X['cluster'], X[col])
    if table.shape == (2, 2):
        test = Table2x2(table.values)
        # Calculate chi-squared statistic using test.test_nominal_association()
        result = test.test_nominal_association()
        stat = result.statistic  # Access the statistic from the result object
        p = result.pvalue
        print(f"{col:<20} → Chi² = {stat:.2f}, p = {p:.4f}")
    else:
        chi2, p, dof, _ = chi2_contingency(table)
        print(f"{col:<20} → Chi² = {chi2:.2f}, p = {p:.4f}")

# Optional: Post-hoc pairwise comparisons (Tukey) for time
print("\n Tukey HSD post-hoc for 'time':")
tukey_df = X[['time', 'cluster']].copy()
tukey_df['cluster'] = tukey_df['cluster'].astype(str)
tukey = pairwise_tukeyhsd(endog=tukey_df['time'], groups=tukey_df['cluster'], alpha=0.05)
print(tukey.summary())

# Optional: Effect sizes
print("\n Approximate Effect Sizes (for 2 clusters only):\n")

def cohen_d(group1, group2):
    diff = np.mean(group1) - np.mean(group2)
    pooled_std = np.sqrt((np.std(group1, ddof=1) ** 2 + np.std(group2, ddof=1) ** 2) / 2)
    return diff / pooled_std

# Cohen's d for numeric
for col in numeric_cols + ['time']:
    g1 = X[X['cluster'] == 0][col].dropna()
    g2 = X[X['cluster'] == 1][col].dropna()
    d = cohen_d(g1, g2)
    print(f"{col:<20} → Cohen's d = {d:.2f}")

# Cramer's V for categorical
def cramers_v(confusion_matrix):
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    return np.sqrt(phi2 / min(k - 1, r - 1))

print()
for col in categorical_cols + ['cid']:
    table = pd.crosstab(X['cluster'], X[col])
    if table.shape[0] > 1 and table.shape[1] > 1:
        v = cramers_v(table)
        print(f"{col:<20} → Cramér's V = {v:.2f}")

"""See which variables are the most important"""

# Train model
y = X['cid']
X = X.drop(columns=['cid'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
tree = DecisionTreeClassifier(max_depth=4, random_state=42)
tree.fit(X_train, y_train)

importances = pd.Series(tree.feature_importances_, index=X.columns)
importances = importances.sort_values(ascending=False)
plt.figure(figsize=(10, 6))
# Define Blaugrana palette and expand it to match the number of bars
blaugrana_palette = ["#A50044", "#004D98", "#8E0038", "#00295B"]
palette = (blaugrana_palette * ((len(importances) // len(blaugrana_palette)) + 1))[:len(importances)]
sns.barplot(
    x=importances.values,
    y=importances.index,
    # Blaugrana Blue (mid tone)
    color="#004D98"
)
plt.xlabel("Feature Importance for cid")
plt.title("Feature Importance (Decision Tree)")
plt.tight_layout()
plt.show()

# Classification report
y_pred = tree.predict(X_test)
print(classification_report(y_test, y_pred))